{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert - POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBaseMultilingualEmbeddingApi:\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\", cuda=True):\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.tokens_tensor = None\n",
    "        self.segments_tensor = None\n",
    "        \n",
    "        self.encoded_layers_ = None\n",
    "        self.token_embeddings_ = None\n",
    "        \n",
    "    def _tokenize_text(self, text):\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        \n",
    "        self.tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)\n",
    "        self.segments_tensor = torch.tensor([segments_ids]).to(self.device)\n",
    "    \n",
    "    def _evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.model(self.tokens_tensor, self.segments_tensor)\n",
    "        self.encoded_layers_ = encoded_layers\n",
    "    \n",
    "    def _generate_token_embeddings(self, batch_i=0):\n",
    "        \"\"\"\n",
    "        Convert the hidden state embeddings into single token vectors\n",
    "        Holds the list of 12 layer embeddings for each token\n",
    "        Will have the shape: [# tokens, # layers, # features]\n",
    "        \"\"\"\n",
    "        token_embeddings = [] \n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(self.encoded_layers_[-1][batch_i])):\n",
    "            # Holds 12 layers of hidden states for each token \n",
    "            hidden_layers = [] \n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(self.encoded_layers_)):\n",
    "            # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = self.encoded_layers_[layer_i][batch_i][token_i]\n",
    "                hidden_layers.append(vec)\n",
    "            token_embeddings.append(hidden_layers)\n",
    "        self.token_embeddings_ = token_embeddings\n",
    "    \n",
    "    def feed_forward(self, sentence):\n",
    "        self._tokenize_text(sentence)\n",
    "        self._evaluate()\n",
    "        self._generate_token_embeddings()\n",
    "    \n",
    "    def create_word_embedding_(self, how=\"cat_last_4\"):\n",
    "        if how == \"cat_last_4\":\n",
    "            return [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in self.token_embeddings_]\n",
    "        elif how == \"sum_last_4\":\n",
    "            return [torch.sum(torch.stack(layer)[-4:], 0) for layer in self.token_embeddings_]\n",
    "        else:\n",
    "            print(\"Redefine `how` parameter\")\n",
    "        \n",
    "    def create_sentence_embedding_(self, how=\"mean_last_layer\"):\n",
    "        if how == \"mean_last_layer\":\n",
    "            return torch.mean(self.encoded_layers_[-1], 1).squeeze()\n",
    "        elif how == \"mean_cat_last_4_layers\":\n",
    "            return torch.mean(torch.cat((self.encoded_layers_[-1], self.encoded_layers_[-2], self.encoded_layers_[-3], self.encoded_layers_[-4]), 2), 1).squeeze()\n",
    "            #return torch.mean(torch.stack(self.create_word_embedding_(\"cat_last_4\")), 0)\n",
    "        elif how == \"mean_sum_last_4_layers\": \n",
    "            return torch.mean(torch.sum(torch.stack(self.encoded_layers_[-4:]), 0), 1).squeeze()\n",
    "        else:\n",
    "            print(\"Redefine `how` parameter\", how)\n",
    "            \n",
    "    def print_dimensions_(self):\n",
    "        print (\"Number of layers:\", len(self.encoded_layers_))\n",
    "        layer_i = 0\n",
    "        print (\"Number of batches:\", len(self.encoded_layers_[layer_i]))\n",
    "        batch_i = 0\n",
    "        print (\"Number of tokens:\", len(self.encoded_layers_[layer_i][batch_i]))\n",
    "        token_i = 0\n",
    "        print (\"Number of hidden units:\", len(self.encoded_layers_[layer_i][batch_i][token_i]))\n",
    "        \n",
    "    def plot_embedding_hist(self, vec):\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.distplot(vec)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from pprint import pprint\n",
    "NUMBER_OF_FILES_TO_OPEN = 5013\n",
    "CORPUS = \"gazeta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tomasza <Entity name=\"Tomasz Sekielski\" type=\"person\" '\n",
      " 'category=\"dziennikarze\">Sekielskiego</Entity> oburzyła wypowiedź prof. '\n",
      " 'Mariana Filara. Wybitny prawnik stwierdził, że nawet jeśli znalezione w celi '\n",
      " 'Mariusza T. to prowokacja - postąpiono słusznie. - Nie wiem, co się stało '\n",
      " 'profesorowi. Proponuje bardzo niebezpieczną drogę. Mariusz T. nigdy nie '\n",
      " 'powinien wyjść na wolność. Ale nie można w tym celu łamać prawa - mówił '\n",
      " '<Entity name=\"Tomasz Sekielski\" type=\"person\" '\n",
      " 'category=\"dziennikarze\">Sekielski</Entity> w TOK FM. \"Człowiek, który '\n",
      " 'zdecydował o podrzuceniu Mariuszowi T. do celi kompromitujących materiałów, '\n",
      " 'postawił na szali jedne wartości przeciw drugim. Postawił naruszenie prawa '\n",
      " 'przeciw ludzkiemu bezpieczeństwu. W mojej ocenie wybrał słusznie\" - tak w '\n",
      " 'wywiadzie dla \"Gazety Wyborczej\" prof. <Entity name=\"Marian Filar\" '\n",
      " 'type=\"person\" category=\"muzycy\">Marian Filar</Entity> komentował informacje '\n",
      " 'o znalezieniu w celi Mariusza T. materiałów z pornografią dziecięcą. Słowa '\n",
      " 'prawnika zaskoczyły i oburzyły <Entity name=\"Tomasz Sekielski\" type=\"person\" '\n",
      " 'category=\"dziennikarze\">Tomasza Sekielskiego</Entity>. Dziennikarz TVP '\n",
      " 'przyznał, że kiedy rozmowa ukazała się w internecie, przeczytał ją kilka '\n",
      " 'razy. Bo nie mógł uwierzyć, w to co zobaczył. - Nie wiem, co się stało panu '\n",
      " 'profesorowi. Prawnik tak często piętnujący naginanie prawa, wykorzystywanie '\n",
      " 'go do doraźnych celów politycznych, teraz ogłasza oto, że prawo jest '\n",
      " 'nieważne. Że w imię wyimaginowanej wyższej konieczności można nie tylko '\n",
      " 'manipulować prawem, ale także działać poza nim - komentował gospodarz '\n",
      " '\"Poranka Radia TOK FM\". Zdaniem <Entity name=\"Tomasz Sekielski\" '\n",
      " 'type=\"person\" category=\"dziennikarze\">Sekielskiego</Entity> prawnik z UMK w '\n",
      " 'Toruniu proponuje \"bardzo niebezpieczną drogę\". - Prowadzącą wprost do '\n",
      " 'państwa totalitarnego, w którym jakiś urzędnik lub polityk mogą dowolnie i '\n",
      " 'bezkarnie decydować o czyimś życiu. Tak na marginesie, może od razu '\n",
      " 'należałoby powołać szwadrony śmierci, by ostatecznie rozwiązywałyby '\n",
      " 'problemy. Dziennikarz nie ma wątpliwości - Mariusz T. nie powinien nigdy '\n",
      " 'wyjść z więzienia. - Ale nie można w tym celu łamać prawa, bo za chwilę w '\n",
      " 'podobnej sytuacji może się znaleźć nie tylko morderca-pedofil - powiedział '\n",
      " '<Entity name=\"Tomasz Sekielski\" type=\"person\" category=\"dziennikarze\">Tomasz '\n",
      " 'Sekielski</Entity>.')\n",
      "5013\n"
     ]
    }
   ],
   "source": [
    "corpusNews = list()\n",
    "for i in range(0,NUMBER_OF_FILES_TO_OPEN):\n",
    "    try:\n",
    "        f = codecs.open(\"doc/\"+ CORPUS +\"/doc\"+str(i+1), \"r\", encoding = 'utf-8')\n",
    "        lines = \"\"\n",
    "        for line in f:\n",
    "            lines += line\n",
    "        corpusNews.append(lines)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        with open(\"doc/\"+ CORPUS +\"/doc\"+str(i+1), \"w\", encoding = 'utf-8') as missing:\n",
    "            pass\n",
    "        corpusNews.append(\"\")\n",
    "        \n",
    "pprint(corpusNews[0])\n",
    "print(len(corpusNews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions from Piter, that I did not changed (but use it differently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_annotations(document : str):\n",
    "    '''\n",
    "    Searches for all occurances of '<' and '>' in the document.\n",
    "    Returns lists of indexes of occurances opening for '<' and closing for '>'\n",
    "    '''\n",
    "    i = 0\n",
    "    opening = list()\n",
    "    closing = list()\n",
    "    while i != -1:\n",
    "        i = document.find('<', i)\n",
    "        opening.append(i)\n",
    "        if i == -1:\n",
    "            closing.append(-1)\n",
    "            break\n",
    "        i = document.find('>', i)\n",
    "        closing.append(i)\n",
    "    closing = [cl + 1 for cl in closing]\n",
    "    closing[-1] = -1\n",
    "    return(opening, closing)\n",
    "\n",
    "def get_annotation_values(text : str): # jest typ nie type, bo type jest zarezerwowana nazwa, nie jestem uposledzony\n",
    "    '''\n",
    "    Returns a dict consisting annotation values {'name', 'typ', 'category'} for first occuring annotation\n",
    "    in the text.\n",
    "    '''\n",
    "    name_start = text.find('name=') + len('name=\\\"')\n",
    "    name_end = text.find('\\\"', name_start)\n",
    "    typ_start = text.find('type=', name_end) + len('type=\\\"')\n",
    "    typ_end = text.find('\\\"', typ_start)\n",
    "    category_start = text.find('category', typ_end) + len('category=\\\"')\n",
    "    category_end = text.find('\\\"', category_start)\n",
    "    return({'name' : text[name_start:name_end], 'typ' : text[typ_start:typ_end],\n",
    "               'category' : text[category_start:category_end]})\n",
    "\n",
    "def exclude_vectors_nsize(text, nsize = 3):\n",
    "    '''\n",
    "    Parameters:\n",
    "    text - document string\n",
    "    nsize - size of window around person word \n",
    "    Returns a list of lists build as follows [k_words before person, person, k_words after person], person name]\n",
    "    '''\n",
    "    global people_dict\n",
    "    opn, cls = find_annotations(text)\n",
    "    ind = 0\n",
    "    l = list()\n",
    "    for i in range(0, len(opn) - 1, 2):\n",
    "        left_sentence = last_n(text[ind:opn[i]].split(' '), nsize)\n",
    "        left_sentence = repair_sentence(left_sentence, nsize, left = True)\n",
    "        right_sentence = first_n(text[cls[i+1]:text.find('<', cls[i+1])].split(' ') , nsize)\n",
    "        right_sentence = repair_sentence(right_sentence, nsize, left = False)\n",
    "        annotation = get_annotation_values(text[ind:-1])\n",
    "        l.append([flat_list([left_sentence, right_sentence]), annotation[\"name\"]])\n",
    "        try:\n",
    "            people_dict[annotation[\"category\"]].add(annotation[\"name\"])\n",
    "        except KeyError:\n",
    "            people_dict[annotation[\"category\"]] = {annotation[\"name\"]}\n",
    "        ind = cls[i+1]\n",
    "    return l\n",
    "\n",
    "def exclude_vectors_for_person(list_of_vectors, person):\n",
    "    '''\n",
    "    list_of_vectors - return value from exclude_sentence_vectors()\n",
    "    person - name of person from people_dict\n",
    "    '''\n",
    "    l = list()\n",
    "    for el in list_of_vectors:\n",
    "        if el[1] == person:\n",
    "            l.append(el[0])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More functions from Piter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(text : str):\n",
    "    '''\n",
    "    Returns reversed text.\n",
    "    '''\n",
    "    return(text[::-1])\n",
    "\n",
    "def last_n(_list, n):\n",
    "    '''\n",
    "    Returns last n elements of list. \n",
    "    Returns full list if n is greater than list length or empty string if list is empty\n",
    "    '''\n",
    "    if not _list:\n",
    "        return('')\n",
    "    if _list[len(_list) - 1] == '':\n",
    "        _list.pop()\n",
    "    return(_list[-n:])\n",
    "\n",
    "def first_n(_list, n):\n",
    "    '''\n",
    "    Returns first n elements of list. \n",
    "    Returns full list if n is greater than list length or empty string if list is empty\n",
    "    '''\n",
    "    if not _list:\n",
    "        return('')\n",
    "    if _list[0] == '':\n",
    "        _list.pop(0)\n",
    "    return(_list[:n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_dot = ['m.in.', 'inż.', 'prof.', 'tzn.', 'np.', 'cd.', 'al.', 'cnd.', \n",
    "                  'itp.', 'itd.', 'lek.', 'lic.', 'pl.', 'p.o.', 'św.', 'tj.', \n",
    "                  'tzw.', 'ul.', 'zob.', 'ul.']\n",
    "\n",
    "punctuation = ['\"', ',', '.', ':', '(', ')', '?', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_the_word(word = '\"Pas.chanacz:lolo)u(marek,pies!?\"'):\n",
    "    '''\n",
    "    Splits the word with elements in punctuation list. Words in words_with_dot are excluded from splitting.\n",
    "    Returns a list of splitted word. Splitting characters are included in the list.\n",
    "    '''\n",
    "    global words_with_dot\n",
    "    global punctuation\n",
    "    if word in words_with_dot:\n",
    "        return([word])\n",
    "    else:\n",
    "        l = list()\n",
    "        index_start = 0\n",
    "        index_end = 0\n",
    "        for i, char in enumerate(word):\n",
    "            if char in punctuation:\n",
    "                if index_start != index_end:\n",
    "                    l.append(word[index_start:index_end])\n",
    "                l.append(char)\n",
    "                index_start = i + 1\n",
    "                index_end = i + 1\n",
    "            else:\n",
    "                index_end = i + 1\n",
    "        if index_start != index_end:\n",
    "            l.append(word[index_start:index_end])\n",
    "        return(l)\n",
    "    \n",
    "def flat_list(_list): # https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "    '''\n",
    "    Create one list from list of lists.\n",
    "    '''\n",
    "    flat_list = []\n",
    "    for sublist in _list:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return(flat_list)\n",
    "    \n",
    "def repair_sentence(_list, nsize, left):\n",
    "    '''\n",
    "    Naprawia zdanie xD. Chodzi o to, zeby oddzielic znaki interpunkcyjne.\n",
    "    '''\n",
    "    global words_with_dot\n",
    "    l = list()\n",
    "    for el in _list:\n",
    "        if el not in words_with_dot:\n",
    "            l.append(split_the_word(el))\n",
    "        else:\n",
    "            l.append([el])\n",
    "    if left:\n",
    "        return(last_n(flat_list(l), nsize))\n",
    "    else:\n",
    "        return(first_n(flat_list(l), nsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_dict = {}\n",
    "\n",
    "def exclude_vectors_nsize(text, nsize = 3):\n",
    "    '''\n",
    "    Parameters:\n",
    "    text - document string\n",
    "    nsize - size of window around person word \n",
    "    Returns a list of lists build as follows [k_words before person, person, k_words after person], person name]\n",
    "    '''\n",
    "    global people_dict\n",
    "    opn, cls = find_annotations(text)\n",
    "    ind = 0\n",
    "    l = list()\n",
    "    for i in range(0, len(opn) - 1, 2):\n",
    "        left_sentence = last_n(text[ind:opn[i]].split(' '), nsize)\n",
    "        left_sentence = repair_sentence(left_sentence, nsize, left = True)\n",
    "        right_sentence = first_n(text[cls[i+1]:text.find('<', cls[i+1])].split(' ') , nsize)\n",
    "        right_sentence = repair_sentence(right_sentence, nsize, left = False)\n",
    "        annotation = get_annotation_values(text[ind:-1])\n",
    "        l.append([flat_list([left_sentence, right_sentence]), annotation[\"name\"]])\n",
    "        try:\n",
    "            people_dict[annotation[\"category\"]].add(annotation[\"name\"])\n",
    "        except KeyError:\n",
    "            people_dict[annotation[\"category\"]] = {annotation[\"name\"]}\n",
    "        ind = cls[i+1]\n",
    "    return l, people_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our function for sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_sentence_vectors(text):\n",
    "    people_dict = {}\n",
    "    sentences = text.split(\".\")\n",
    "    contexts = []\n",
    "    for sentence in sentences:\n",
    "        opn, cls = find_annotations(sentence)\n",
    "        if opn[0] != -1:\n",
    "            #print(opn, cls)\n",
    "            annotation = get_annotation_values(sentence)\n",
    "            sentence = sentence[0:opn[0]] + sentence[cls[-2]:]\n",
    "            contexts.append([sentence, annotation[\"name\"]])\n",
    "            try:\n",
    "                people_dict[annotation[\"category\"]].add(annotation[\"name\"])\n",
    "            except KeyError:\n",
    "                people_dict[annotation[\"category\"]] = {annotation[\"name\"]}\n",
    "    return contexts, people_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectors, people_dict = exclude_sentence_vectors(corpusNews[0])\n",
    "print(vectors)\n",
    "print(people_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Tomasza', 'oburzyła', 'wypowiedź', 'prof.'], 'Tomasz Sekielski'], [['prawa', '-', 'mówił', 'w', 'TOK', 'FM'], 'Tomasz Sekielski'], [['Wyborczej', '\"', 'prof.', 'komentował', 'informacje', 'o'], 'Marian Filar'], [['zaskoczyły', 'i', 'oburzyły', '.', 'Dziennikarz', 'TVP'], 'Tomasz Sekielski'], [['\"', '.', 'Zdaniem', 'prawnik', 'z', 'UMK'], 'Tomasz Sekielski'], [['morderca-pedofil', '-', 'powiedział'], 'Tomasz Sekielski']]\n",
      "{'dziennikarze': {'Tomasz Sekielski'}, 'muzycy': {'Marian Filar'}}\n",
      "[['Tomasza', 'oburzyła', 'wypowiedź', 'prof.'], ['prawa', '-', 'mówił', 'w', 'TOK', 'FM'], ['zaskoczyły', 'i', 'oburzyły', '.', 'Dziennikarz', 'TVP'], ['\"', '.', 'Zdaniem', 'prawnik', 'z', 'UMK'], ['morderca-pedofil', '-', 'powiedział']]\n"
     ]
    }
   ],
   "source": [
    "vectors, people_dict = exclude_vectors_nsize(corpusNews[0])\n",
    "print(vectors) \n",
    "print(people_dict)\n",
    "print(exclude_vectors_for_person(vectors, \"Tomasz Sekielski\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[['Tomasza', 'oburzyła', 'wypowiedź', 'prof.', 'Mariana'],\n",
      "   'Tomasz Sekielski'],\n",
      "  [['łamać', 'prawa', '-', 'mówił', 'w', 'TOK', 'FM', '.'], 'Tomasz Sekielski'],\n",
      "  [['Gazety',\n",
      "    'Wyborczej',\n",
      "    '\"',\n",
      "    'prof.',\n",
      "    'komentował',\n",
      "    'informacje',\n",
      "    'o',\n",
      "    'znalezieniu'],\n",
      "   'Marian Filar'],\n",
      "  [['prawnika',\n",
      "    'zaskoczyły',\n",
      "    'i',\n",
      "    'oburzyły',\n",
      "    '.',\n",
      "    'Dziennikarz',\n",
      "    'TVP',\n",
      "    'przyznał'],\n",
      "   'Tomasz Sekielski'],\n",
      "  [['FM', '\"', '.', 'Zdaniem', 'prawnik', 'z', 'UMK', 'w'], 'Tomasz Sekielski'],\n",
      "  [['tylko', 'morderca-pedofil', '-', 'powiedział'], 'Tomasz Sekielski']],\n",
      " {'dziennikarze': {'Tomasz Sekielski'}, 'muzycy': {'Marian Filar'}})\n"
     ]
    }
   ],
   "source": [
    "vectors = exclude_vectors_nsize(corpusNews[0], 4)\n",
    "pprint(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dziennikarze': {'Tomasz Sekielski'}, 'muzycy': {'Marian Filar'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Name\"\n",
    "EMBEDDING = \"embedding\"\n",
    "PROFESSION = \"Profession\"\n",
    "TYPE = \"mean_cat_last_4_layers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_Piter(news_file):\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    vectors, people_dict = exclude_vectors_nsize(news_file, 4)\n",
    "    for profession in people_dict:\n",
    "        names = people_dict[profession]\n",
    "        for name in names:\n",
    "            person_contexts = exclude_vectors_for_person(vectors, name)\n",
    "            for context in person_contexts:\n",
    "                concatenated = \" \".join(c for c in context)\n",
    "                bert.feed_forward(concatenated)\n",
    "                sent_embedding = bert.create_sentence_embedding_(how = TYPE)\n",
    "                vec = pd.DataFrame(sent_embedding.cpu().numpy()).T\n",
    "                embeddings.append(vec)\n",
    "                metadata.append({NAME: name, PROFESSION: profession})\n",
    "    return embeddings, metadata\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(news_file):\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    vectors, people_dict = exclude_sentence_vectors(news_file)\n",
    "    for profession in people_dict:\n",
    "        names = people_dict[profession]\n",
    "        for name in names:\n",
    "            person_contexts = exclude_vectors_for_person(vectors, name)\n",
    "            for context in person_contexts:\n",
    "                bert.feed_forward(context)\n",
    "                sent_embedding = bert.create_sentence_embedding_(how = TYPE)\n",
    "                vec = pd.DataFrame(sent_embedding.cpu().numpy()).T\n",
    "                embeddings.append(vec)\n",
    "                metadata.append({NAME: name, PROFESSION: profession})\n",
    "    return  embeddings, metadata\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_tsv_files(filename, embeddings):\n",
    "#     with open(filename + \"_vectors.tsv\",'w') as f:\n",
    "#         for vector in embeddings:\n",
    "#             for value in vector[EMBEDDING]:\n",
    "#                 f.write(\"%e\\t\" % (value))\n",
    "#             f.write(\"\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     with open(filename + \"_metadata.tsv\",'w') as f:\n",
    "#         f.write(\"Name\\tProfession\\n\")\n",
    "#         for vector in embeddings:\n",
    "#             f.write(\"%s\\t%s\\n\" % (vector[NAME], vector[PROFESSION]))\n",
    "#     f.close()\n",
    "\n",
    "#generate_tsv_files(\"tsv_files/\"+ TYPE +\"/\" + CORPUS +\"/\" + TYPE + \"_doc\"+str(i+1), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vextors_meta_df(number_of_layers=4):\n",
    "    vectors_meta_df = pd.DataFrame(columns=range(number_of_layers*768+3)).rename(columns=\n",
    "        {\n",
    "        number_of_layers*768: \"Name\",\n",
    "        number_of_layers*768+1: \"Profession\",\n",
    "        number_of_layers*768+2: \"Document\"\n",
    "        })\n",
    "    return vectors_meta_df\n",
    "\n",
    "def generate_concat_vectors_meta_df(number_of_files):\n",
    "    vectors_meta_df = create_vextors_meta_df()\n",
    "    for i in tqdm(range(0, number_of_files)):\n",
    "        #pprint(corpusNews[i])\n",
    "        embeddings, metadata = generate_embeddings_Piter(corpusNews[i])\n",
    "        vec = pd.DataFrame(embeddings)\n",
    "        joined = vec.join(pd.DataFrame(metadata))\n",
    "        joined[\"Document\"] = \"doc\"+ str(i+1)\n",
    "        vectors_meta_df = vectors_meta_df.append(joined)\n",
    "    return vectors_meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5013/5013 [07:24<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df = generate_concat_vectors_meta_df(NUMBER_OF_FILES_TO_OPEN)\n",
    "df.to_csv(\"tsv_files/\" + TYPE +\"/\" + CORPUS +\"/results_words.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Name</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>3062</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1</td>\n",
       "      <td>dziennikarze</td>\n",
       "      <td>Tomasz Sekielski</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>0.058421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1</td>\n",
       "      <td>muzycy</td>\n",
       "      <td>Marian Filar</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "      <td>-0.185113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc1002</td>\n",
       "      <td>duchowni</td>\n",
       "      <td>Wojciech Polak</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.148669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1006</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Janusz Korwin-Mikke</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>-0.050047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc1006</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Jarosław Gowin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.004990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4149</th>\n",
       "      <td>doc995</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Bogdan Zdrojewski</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "      <td>-0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4150</th>\n",
       "      <td>doc997</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Antoni Macierewicz</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "      <td>-0.074773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>doc997</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Donald Tusk</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>-0.038954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>doc997</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Janusz Palikot</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.038415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>doc997</td>\n",
       "      <td>politycy</td>\n",
       "      <td>Rafał Grupiński</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.090011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4154 rows × 3076 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Document    Profession                 Name  Unnamed: 0         0  \\\n",
       "0        doc1  dziennikarze     Tomasz Sekielski         2.0  0.058421   \n",
       "1        doc1        muzycy         Marian Filar         5.0 -0.185113   \n",
       "2     doc1002      duchowni       Wojciech Polak         0.0  0.148669   \n",
       "3     doc1006      politycy  Janusz Korwin-Mikke         1.0 -0.050047   \n",
       "4     doc1006      politycy       Jarosław Gowin         0.0 -0.004990   \n",
       "...       ...           ...                  ...         ...       ...   \n",
       "4149   doc995      politycy    Bogdan Zdrojewski         1.5 -0.003693   \n",
       "4150   doc997      politycy   Antoni Macierewicz         7.0 -0.074773   \n",
       "4151   doc997      politycy          Donald Tusk         4.5 -0.038954   \n",
       "4152   doc997      politycy       Janusz Palikot         2.5  0.038415   \n",
       "4153   doc997      politycy      Rafał Grupiński         0.5  0.090011   \n",
       "\n",
       "             1         2         3         4         5  ...      3062  \\\n",
       "0     0.058421  0.058421  0.058421  0.058421  0.058421  ...  0.058421   \n",
       "1    -0.185113 -0.185113 -0.185113 -0.185113 -0.185113  ... -0.185113   \n",
       "2     0.148669  0.148669  0.148669  0.148669  0.148669  ...  0.148669   \n",
       "3    -0.050047 -0.050047 -0.050047 -0.050047 -0.050047  ... -0.050047   \n",
       "4    -0.004990 -0.004990 -0.004990 -0.004990 -0.004990  ... -0.004990   \n",
       "...        ...       ...       ...       ...       ...  ...       ...   \n",
       "4149 -0.003693 -0.003693 -0.003693 -0.003693 -0.003693  ... -0.003693   \n",
       "4150 -0.074773 -0.074773 -0.074773 -0.074773 -0.074773  ... -0.074773   \n",
       "4151 -0.038954 -0.038954 -0.038954 -0.038954 -0.038954  ... -0.038954   \n",
       "4152  0.038415  0.038415  0.038415  0.038415  0.038415  ...  0.038415   \n",
       "4153  0.090011  0.090011  0.090011  0.090011  0.090011  ...  0.090011   \n",
       "\n",
       "          3063      3064      3065      3066      3067      3068      3069  \\\n",
       "0     0.058421  0.058421  0.058421  0.058421  0.058421  0.058421  0.058421   \n",
       "1    -0.185113 -0.185113 -0.185113 -0.185113 -0.185113 -0.185113 -0.185113   \n",
       "2     0.148669  0.148669  0.148669  0.148669  0.148669  0.148669  0.148669   \n",
       "3    -0.050047 -0.050047 -0.050047 -0.050047 -0.050047 -0.050047 -0.050047   \n",
       "4    -0.004990 -0.004990 -0.004990 -0.004990 -0.004990 -0.004990 -0.004990   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4149 -0.003693 -0.003693 -0.003693 -0.003693 -0.003693 -0.003693 -0.003693   \n",
       "4150 -0.074773 -0.074773 -0.074773 -0.074773 -0.074773 -0.074773 -0.074773   \n",
       "4151 -0.038954 -0.038954 -0.038954 -0.038954 -0.038954 -0.038954 -0.038954   \n",
       "4152  0.038415  0.038415  0.038415  0.038415  0.038415  0.038415  0.038415   \n",
       "4153  0.090011  0.090011  0.090011  0.090011  0.090011  0.090011  0.090011   \n",
       "\n",
       "          3070      3071  \n",
       "0     0.058421  0.058421  \n",
       "1    -0.185113 -0.185113  \n",
       "2     0.148669  0.148669  \n",
       "3    -0.050047 -0.050047  \n",
       "4    -0.004990 -0.004990  \n",
       "...        ...       ...  \n",
       "4149 -0.003693 -0.003693  \n",
       "4150 -0.074773 -0.074773  \n",
       "4151 -0.038954 -0.038954  \n",
       "4152  0.038415  0.038415  \n",
       "4153  0.090011  0.090011  \n",
       "\n",
       "[4154 rows x 3076 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"tsv_files/\" + TYPE +\"/\" + CORPUS +\"/results_words.tsv\", sep=\"\\t\")\n",
    "df.groupby([\"Document\", \"Profession\", \"Name\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create API object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertBaseMultilingualEmbeddingApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create sentence and pass to feed_forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Oszczędnością i pracą ludzie się bogacą.\"\n",
    "bert.feed_forward(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate word/sentence embedding, specify `how`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = bert.create_word_embedding_(how = \"sum_last_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = bert.create_sentence_embedding_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Visualize tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.plot_embedding_hist(sent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = 'minimalsample'\n",
    "NAME_TO_VISUALISE_VARIABLE = \"example_embeddings\"\n",
    "path_for_mnist_metadata =  'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings_np = torch.stack(concatenated_lats_4_layers).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_var = tf.Variable(token_embeddings_np, name=NAME_TO_VISUALISE_VARIABLE)\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Specify where you find the metadata\n",
    "embedding.metadata_path = path_for_mnist_metadata #'metadata.tsv'\n",
    "\n",
    "# Say that you want to visualise the embeddings\n",
    "projector.visualize_embeddings(summary_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_for_mnist_metadata,'w') as f:\n",
    "    f.write(\"Index\\tLabel\\n\")\n",
    "    for index,label in enumerate(tokenized_text):\n",
    "        f.write(\"%d\\t %s\\n\" % (index,label.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
