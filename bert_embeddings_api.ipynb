{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert - POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBaseMultilingualEmbeddingApi:\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\", cuda=True):\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.tokens_tensor = None\n",
    "        self.segments_tensor = None\n",
    "        \n",
    "        self.encoded_layers_ = None\n",
    "        self.token_embeddings_ = None\n",
    "        \n",
    "    def _tokenize_text(self, text):\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        \n",
    "        self.tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)\n",
    "        self.segments_tensor = torch.tensor([segments_ids]).to(self.device)\n",
    "    \n",
    "    def _evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.model(self.tokens_tensor, self.segments_tensor)\n",
    "        self.encoded_layers_ = encoded_layers\n",
    "    \n",
    "    def _generate_token_embeddings(self, batch_i=0):\n",
    "        \"\"\"\n",
    "        Convert the hidden state embeddings into single token vectors\n",
    "        Holds the list of 12 layer embeddings for each token\n",
    "        Will have the shape: [# tokens, # layers, # features]\n",
    "        \"\"\"\n",
    "        token_embeddings = [] \n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(self.encoded_layers_[-1][batch_i])):\n",
    "            # Holds 12 layers of hidden states for each token \n",
    "            hidden_layers = [] \n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(self.encoded_layers_)):\n",
    "            # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = self.encoded_layers_[layer_i][batch_i][token_i]\n",
    "                hidden_layers.append(vec)\n",
    "            token_embeddings.append(hidden_layers)\n",
    "        self.token_embeddings_ = token_embeddings\n",
    "    \n",
    "    def feed_forward(self, sentence):\n",
    "        self._tokenize_text(sentence)\n",
    "        self._evaluate()\n",
    "        self._generate_token_embeddings()\n",
    "    \n",
    "    def create_word_embedding_(self, how=\"cat_last_4\"):\n",
    "        if how == \"cat_last_4\":\n",
    "            return [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in self.token_embeddings_]\n",
    "        elif how == \"sum_last_4\":\n",
    "            return [torch.sum(torch.stack(layer)[-4:], 0) for layer in self.token_embeddings_]\n",
    "        else:\n",
    "            print(\"Redefine `how` parameter\")\n",
    "        \n",
    "    def create_sentence_embedding_(self, how=\"mean_last_layer\"):\n",
    "        if how == \"mean_last_layer\":\n",
    "            return torch.mean(self.encoded_layers_[-1], 1).squeeze()\n",
    "        else:\n",
    "            print(\"Redefine `how` parameter\")\n",
    "            \n",
    "    def print_dimensions_(self):\n",
    "        print (\"Number of layers:\", len(self.encoded_layers_))\n",
    "        layer_i = 0\n",
    "        print (\"Number of batches:\", len(self.encoded_layers_[layer_i]))\n",
    "        batch_i = 0\n",
    "        print (\"Number of tokens:\", len(self.encoded_layers_[layer_i][batch_i]))\n",
    "        token_i = 0\n",
    "        print (\"Number of hidden units:\", len(self.encoded_layers_[layer_i][batch_i][token_i]))\n",
    "        \n",
    "    def plot_embedding_hist(self, vec):\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.distplot(vec)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from pprint import pprint\n",
    "NUMBER_OF_FILES_TO_OPEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tomasza <Entity name=\"Tomasz Sekielski\" type=\"person\" '\n",
      " 'category=\"dziennikarze\">Sekielskiego</Entity> oburzyła wypowiedź prof. '\n",
      " 'Mariana Filara. Wybitny prawnik stwierdził, że nawet jeśli znalezione w celi '\n",
      " 'Mariusza T. to prowokacja - postąpiono słusznie. - Nie wiem, co się stało '\n",
      " 'profesorowi. Proponuje bardzo niebezpieczną drogę. Mariusz T. nigdy nie '\n",
      " 'powinien wyjść na wolność. Ale nie można w tym celu łamać prawa - mówił '\n",
      " '<Entity name=\"Tomasz Sekielski\" type=\"person\" '\n",
      " 'category=\"dziennikarze\">Sekielski</Entity> w TOK FM. \"Człowiek, który '\n",
      " 'zdecydował o podrzuceniu Mariuszowi T. do celi kompromitujących materiałów, '\n",
      " 'postawił na szali jedne wartości przeciw drugim. Postawił naruszenie prawa '\n",
      " 'przeciw ludzkiemu bezpieczeństwu. W mojej ocenie wybrał słusznie\" - tak w '\n",
      " 'wywiadzie dla \"Gazety Wyborczej\" prof. <Entity name=\"Marian Filar\" '\n",
      " 'type=\"person\" category=\"muzycy\">Marian Filar</Entity> komentował informacje '\n",
      " 'o znalezieniu w celi Mariusza T. materiałów z pornografią dziecięcą. Słowa '\n",
      " 'prawnika zaskoczyły i oburzyły <Entity name=\"Tomasz Sekielski\" type=\"person\" '\n",
      " 'category=\"dziennikarze\">Tomasza Sekielskiego</Entity>. Dziennikarz TVP '\n",
      " 'przyznał, że kiedy rozmowa ukazała się w internecie, przeczytał ją kilka '\n",
      " 'razy. Bo nie mógł uwierzyć, w to co zobaczył. - Nie wiem, co się stało panu '\n",
      " 'profesorowi. Prawnik tak często piętnujący naginanie prawa, wykorzystywanie '\n",
      " 'go do doraźnych celów politycznych, teraz ogłasza oto, że prawo jest '\n",
      " 'nieważne. Że w imię wyimaginowanej wyższej konieczności można nie tylko '\n",
      " 'manipulować prawem, ale także działać poza nim - komentował gospodarz '\n",
      " '\"Poranka Radia TOK FM\". Zdaniem <Entity name=\"Tomasz Sekielski\" '\n",
      " 'type=\"person\" category=\"dziennikarze\">Sekielskiego</Entity> prawnik z UMK w '\n",
      " 'Toruniu proponuje \"bardzo niebezpieczną drogę\". - Prowadzącą wprost do '\n",
      " 'państwa totalitarnego, w którym jakiś urzędnik lub polityk mogą dowolnie i '\n",
      " 'bezkarnie decydować o czyimś życiu. Tak na marginesie, może od razu '\n",
      " 'należałoby powołać szwadrony śmierci, by ostatecznie rozwiązywałyby '\n",
      " 'problemy. Dziennikarz nie ma wątpliwości - Mariusz T. nie powinien nigdy '\n",
      " 'wyjść z więzienia. - Ale nie można w tym celu łamać prawa, bo za chwilę w '\n",
      " 'podobnej sytuacji może się znaleźć nie tylko morderca-pedofil - powiedział '\n",
      " '<Entity name=\"Tomasz Sekielski\" type=\"person\" category=\"dziennikarze\">Tomasz '\n",
      " 'Sekielski</Entity>.')\n"
     ]
    }
   ],
   "source": [
    "corpusNews = list()\n",
    "for i in range(0,NUMBER_OF_FILES_TO_OPEN):\n",
    "    f = codecs.open(\"doc/doc\"+str(i+1), \"r\", encoding = 'utf-8')\n",
    "    for line in f:\n",
    "        corpusNews.append(line)\n",
    "    f.close()\n",
    "pprint(corpusNews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions from Piter, that I did not changed (but use it differently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_annotations(document : str):\n",
    "    '''\n",
    "    Searches for all occurances of '<' and '>' in the document.\n",
    "    Returns lists of indexes of occurances opening for '<' and closing for '>'\n",
    "    '''\n",
    "    i = 0\n",
    "    opening = list()\n",
    "    closing = list()\n",
    "    while i != -1:\n",
    "        i = document.find('<', i)\n",
    "        opening.append(i)\n",
    "        if i == -1:\n",
    "            closing.append(-1)\n",
    "            break\n",
    "        i = document.find('>', i)\n",
    "        closing.append(i)\n",
    "    closing = [cl + 1 for cl in closing]\n",
    "    closing[-1] = -1\n",
    "    return(opening, closing)\n",
    "\n",
    "def get_annotation_values(text : str): # jest typ nie type, bo type jest zarezerwowana nazwa, nie jestem uposledzony\n",
    "    '''\n",
    "    Returns a dict consisting annotation values {'name', 'typ', 'category'} for first occuring annotation\n",
    "    in the text.\n",
    "    '''\n",
    "    name_start = text.find('name=') + len('name=\\\"')\n",
    "    name_end = text.find('\\\"', name_start)\n",
    "    typ_start = text.find('type=', name_end) + len('type=\\\"')\n",
    "    typ_end = text.find('\\\"', typ_start)\n",
    "    category_start = text.find('category', typ_end) + len('category=\\\"')\n",
    "    category_end = text.find('\\\"', category_start)\n",
    "    return({'name' : text[name_start:name_end], 'typ' : text[typ_start:typ_end],\n",
    "               'category' : text[category_start:category_end]})\n",
    "\n",
    "def exclude_vectors_for_person(list_of_vectors, person):\n",
    "    '''\n",
    "    list_of_vectors - return value from exclude_sentence_vectors()\n",
    "    person - name of person from people_dict\n",
    "    '''\n",
    "    l = list()\n",
    "    for el in list_of_vectors:\n",
    "        if el[1] == person:\n",
    "            l.append(el[0])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our function for sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_sentence_vectors(text):\n",
    "    people_dict = {}\n",
    "    sentences = text.split(\".\")\n",
    "    contexts = []\n",
    "    for sentence in sentences:\n",
    "        opn, cls = find_annotations(sentence)\n",
    "        if opn[0] != -1:\n",
    "            annotation = get_annotation_values(sentence)\n",
    "            sentence = sentence[0:opn[0]] + sentence[cls[-2]:]\n",
    "            contexts.append([sentence, annotation[\"name\"]])\n",
    "            try:\n",
    "                people_dict[annotation[\"category\"]].add(annotation[\"name\"])\n",
    "            except KeyError:\n",
    "                people_dict[annotation[\"category\"]] = {annotation[\"name\"]}\n",
    "    return contexts, people_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tomasza  oburzyła wypowiedź prof', 'Tomasz Sekielski'], [' Ale nie można w tym celu łamać prawa - mówił  w TOK FM', 'Tomasz Sekielski'], ['  komentował informacje o znalezieniu w celi Mariusza T', 'Marian Filar'], [' Słowa prawnika zaskoczyły i oburzyły ', 'Tomasz Sekielski'], [' Zdaniem  prawnik z UMK w Toruniu proponuje \"bardzo niebezpieczną drogę\"', 'Tomasz Sekielski'], [' - Ale nie można w tym celu łamać prawa, bo za chwilę w podobnej sytuacji może się znaleźć nie tylko morderca-pedofil - powiedział ', 'Tomasz Sekielski']]\n",
      "{'dziennikarze': {'Tomasz Sekielski'}, 'muzycy': {'Marian Filar'}}\n"
     ]
    }
   ],
   "source": [
    "vectors, people_dict = exclude_sentence_vectors(corpusNews[0])\n",
    "print(vectors)\n",
    "print(people_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tomasza  oburzyła wypowiedź prof', ' Ale nie można w tym celu łamać prawa - mówił  w TOK FM', ' Słowa prawnika zaskoczyły i oburzyły ', ' Zdaniem  prawnik z UMK w Toruniu proponuje \"bardzo niebezpieczną drogę\"', ' - Ale nie można w tym celu łamać prawa, bo za chwilę w podobnej sytuacji może się znaleźć nie tylko morderca-pedofil - powiedział ']\n"
     ]
    }
   ],
   "source": [
    "print(exclude_vectors_for_person(vectors, \"Tomasz Sekielski\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(news_file):\n",
    "    embeddings = []\n",
    "    vectors, people_dict = exclude_sentence_vectors(news_file)\n",
    "    for profession in people_dict:\n",
    "        names = people_dict[profession]\n",
    "        for name in names:\n",
    "            person_contexts = exclude_vectors_for_person(vectors, name)\n",
    "            for context in person_contexts:\n",
    "                bert.feed_forward(context)\n",
    "                sent_embedding = bert.create_sentence_embedding_()\n",
    "                embeddings.append({\"embedding\":sent_embedding, \"name\": name, \"profession\": profession})\n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tsv_files(filename, embeddings):\n",
    "    with open(filename + \"_vectors.tsv\",'w') as f:\n",
    "        for vector in embeddings:\n",
    "            for value in vector[\"embedding\"]:\n",
    "                f.write(\"%e\\t\" % (value))\n",
    "            f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    with open(filename + \"_metadata.tsv\",'w') as f:\n",
    "        f.write(\"Name\\tProfession\\n\")\n",
    "        for vector in embeddings:\n",
    "            f.write(\"%s\\t%s\\n\" % (vector[\"name\"].encode('utf-8'), vector[\"profession\"].encode('utf-8')))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, NUMBER_OF_FILES_TO_OPEN):\n",
    "    embeddings = generate_embeddings(corpusNews[i])\n",
    "    generate_tsv_files(\"tsv_files/doc\"+str(i+1), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create API object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "bert = BertBaseMultilingualEmbeddingApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create sentence and pass to feed_forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Oszczędnością i pracą ludzie się bogacą.\"\n",
    "bert.feed_forward(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate word/sentence embedding, specify `how`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = bert.create_word_embedding_(how = \"sum_last_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = bert.create_sentence_embedding_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Visualize tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.7999e-02, -1.9141e-01,  7.3391e-01,  1.9257e-01,  3.7371e-01,\n",
      "         4.9427e-01, -2.4747e-01, -1.9059e-01,  2.3879e-02,  5.4254e-02,\n",
      "         1.4454e-01,  4.9810e-01,  3.0980e-02,  2.1776e-01,  3.4605e-02,\n",
      "         1.9929e-02,  5.7315e-01, -6.9925e-01,  1.0205e-01,  1.9255e-01,\n",
      "         1.4388e-01,  7.1202e-02, -2.9616e-03,  1.2551e-01, -9.6333e-02,\n",
      "         2.8764e-01, -2.8002e-01, -1.3754e-01, -1.2272e-01, -5.4620e-01,\n",
      "         2.0109e-01,  4.3742e-02, -3.2613e-01,  3.2596e-01, -2.4226e-02,\n",
      "        -1.1224e-01, -3.0505e-01,  2.9660e-01,  3.9777e-01, -2.0974e-01,\n",
      "        -3.0772e-01,  2.6053e-01,  2.2329e-01,  1.4007e-01,  2.5690e-01,\n",
      "        -1.6820e-01,  1.1059e-01, -3.5593e-03, -3.1462e-01,  8.8268e-02,\n",
      "        -1.5128e-02, -1.2418e-01,  9.7928e-02, -2.7980e-02,  6.9080e-02,\n",
      "         2.6530e-01,  8.2499e-02, -9.7751e-02,  1.5672e-01,  1.1794e-01,\n",
      "        -1.2886e-01,  7.4442e-04, -4.3342e-01, -6.0595e-02, -1.7476e-01,\n",
      "         3.7781e-01, -6.0381e-02, -2.3784e-01,  2.4969e-01,  2.3411e-01,\n",
      "        -1.0409e-01, -1.0874e-02,  2.7081e-01,  2.0727e-01, -2.8680e-02,\n",
      "         6.2389e-01,  2.0394e-01, -5.9295e-02, -5.5675e-01, -4.9193e-01,\n",
      "         4.5835e-01,  2.0194e-01, -7.2451e-02,  1.2073e-01, -1.3308e-03,\n",
      "         5.7842e-01,  4.0609e-01,  1.8317e-01,  5.7345e-01, -5.7573e-01,\n",
      "        -2.8537e-02,  1.1382e-02,  1.0179e+00,  7.8593e-02, -4.0681e-01,\n",
      "         5.2580e-01,  3.9443e-01,  2.7415e-01,  2.2367e-01,  3.5298e-01,\n",
      "        -3.1064e-01, -1.2426e-02,  3.9805e-01,  7.9640e-02, -4.9438e-02,\n",
      "        -1.8605e-01,  2.0557e-01,  2.2724e-01,  2.3189e-01,  2.9411e-01,\n",
      "         2.5021e-01,  6.1630e-02,  2.4491e-01, -1.0407e-01,  3.5911e-01,\n",
      "        -2.4812e-01, -5.5143e-01,  6.2117e-01,  3.9339e-01, -1.2846e-02,\n",
      "        -1.5185e-01, -3.4868e-02, -5.5529e-01,  2.2510e-01,  1.7968e-01,\n",
      "        -2.0681e-01,  8.0590e-02, -4.2027e-02,  2.8420e-01,  9.4469e-02,\n",
      "        -3.7403e-01, -4.0628e-02, -1.5292e-02,  9.9663e-02,  4.8220e-01,\n",
      "        -1.8404e-01,  9.1393e-02, -2.1563e-01, -1.7290e-01,  5.1618e-01,\n",
      "        -3.1819e-01,  3.4403e-02, -4.6908e-01, -2.6769e-01, -1.3583e-01,\n",
      "         1.3089e-01, -3.8846e-01,  5.0178e-01, -5.6187e-01, -3.4592e-01,\n",
      "        -3.4566e-01,  5.1994e-01,  6.1983e-01,  2.6089e-01, -3.2610e-01,\n",
      "         1.8050e-01,  5.1125e-01,  1.3471e-01, -4.0939e-01,  3.8223e-01,\n",
      "         3.0396e-01,  2.0124e-01, -3.5128e-01,  9.0220e-02, -3.8160e-01,\n",
      "        -1.4287e-01,  1.4449e-01, -3.0685e-01, -4.5512e-03,  1.8251e-01,\n",
      "        -2.8295e-01,  1.9055e-01, -1.3665e-01,  3.6335e-01,  4.3261e-01,\n",
      "         2.8937e-01,  1.0085e-01, -1.2483e-01,  8.3881e-02, -3.5386e-01,\n",
      "        -2.3438e-01,  1.6019e-01,  2.4489e-02, -1.0380e-01, -2.4781e-01,\n",
      "         1.4270e-01,  2.3153e-01, -1.0297e-01, -1.7086e-01, -1.0971e-01,\n",
      "        -3.3246e-01, -3.5334e-02, -4.3953e-01,  1.1962e-02, -1.7499e-01,\n",
      "         5.0178e-01,  1.2775e-01,  3.8753e-01, -2.7093e-01, -2.4067e-01,\n",
      "        -7.1597e-01,  1.6666e-02, -1.5192e-01,  3.8535e-01, -3.9441e-01,\n",
      "         4.4176e-01, -1.0531e-01, -6.2215e-03,  1.7114e-01,  5.6499e-01,\n",
      "        -3.5758e-01, -1.2246e-01,  1.1698e-01,  1.0017e-01,  2.0275e-01,\n",
      "        -5.0314e-02, -1.4625e-01, -8.7427e-02, -7.6683e-02, -1.9359e-01,\n",
      "         1.6133e-01, -1.1083e-01, -3.6280e-01, -1.7568e-01, -1.5389e-01,\n",
      "        -3.0507e-01, -4.3282e-02,  1.2991e-01, -5.8249e-02, -2.2331e-01,\n",
      "        -4.7625e-01, -7.8963e-02, -2.6140e-01,  4.0464e-01, -2.5171e-01,\n",
      "        -1.9198e-01,  1.6126e-01,  2.5092e-02, -6.1662e-01,  1.2031e-01,\n",
      "        -2.0870e-01,  5.8470e-01,  3.2502e-02, -6.0844e-02, -6.7785e-02,\n",
      "         2.2493e-02,  8.3420e-03, -7.4566e-01,  1.8947e-01, -5.4034e-02,\n",
      "        -2.0207e-01,  2.6877e-01,  2.4332e-01, -2.6946e-01, -8.2815e-03,\n",
      "         3.0483e-02,  2.4237e-03, -2.7413e-01, -1.9320e-02, -1.0463e-01,\n",
      "         3.2536e-01, -5.2175e-01, -6.3491e-02,  1.1347e-02,  8.9506e-02,\n",
      "         6.2552e-02,  1.4657e-01,  3.5609e-01,  3.1517e-02,  2.9919e-01,\n",
      "        -3.1679e-01,  4.5825e-01,  2.2374e-01,  4.0768e-02, -9.6290e-02,\n",
      "         2.9324e-01,  1.5917e-01, -1.9490e-01, -2.5722e-01,  3.3774e-01,\n",
      "         2.1828e-01,  1.5229e-01, -7.4746e-02, -8.2679e-02, -4.3253e-01,\n",
      "        -3.2420e-01,  1.6172e-01, -2.5721e-01,  3.5017e-01, -1.3526e-01,\n",
      "        -3.1574e-01, -6.7980e-01,  1.7287e-01,  2.8237e-01, -5.0172e-02,\n",
      "        -6.4026e-01, -3.5729e-01, -1.4479e-01, -2.5613e-01, -1.8034e-01,\n",
      "         3.5691e-01,  2.2460e-01, -1.3863e-01, -7.5798e-02, -4.6467e-01,\n",
      "         1.7341e-01, -4.3439e-01, -2.5362e-01,  9.4145e-01,  2.1680e-01,\n",
      "        -3.3136e-01,  6.1333e-01, -3.1358e-01,  3.5194e-01,  1.9163e-01,\n",
      "        -5.3275e-02,  3.7099e-01,  9.5384e-03, -3.3290e-01, -2.7794e-02,\n",
      "         4.7965e-02,  4.1400e-02,  4.9903e-01,  1.4432e-01, -2.4906e-01,\n",
      "         5.2010e-02,  3.1263e-02, -4.6675e-01, -7.5414e-02,  6.5705e-02,\n",
      "        -8.3802e-02,  1.2931e+00, -6.6259e-02, -7.2980e-01,  1.1942e-01,\n",
      "        -2.6767e-01, -2.1433e-01,  8.3027e-02, -4.8044e-01, -4.7411e-01,\n",
      "         6.9001e-02, -3.9770e-01,  7.1038e-01, -3.9433e-01,  4.4124e-03,\n",
      "        -2.5032e-01, -4.6571e-01,  8.3702e-01, -2.6220e-02, -5.4740e-01,\n",
      "         1.4199e-01,  1.8148e-01,  1.2686e-01,  3.3611e-01,  1.9860e-01,\n",
      "         2.1265e-01, -3.4167e-01,  9.4555e-02,  3.0502e-01, -2.4452e-01,\n",
      "        -1.2425e-01,  2.0097e-01,  1.7629e-01, -2.0108e-01, -3.7752e-01,\n",
      "         1.2201e-01, -5.9529e-02, -1.0705e-01,  4.0366e-01,  6.9837e-01,\n",
      "         6.8196e-02,  3.6556e-01, -4.8507e-01, -5.6011e-01, -7.0379e-03,\n",
      "         1.9460e-01,  1.9085e-01, -5.2461e-01,  9.3820e-01, -1.9571e-01,\n",
      "         2.8792e-01, -3.3674e-01, -2.9490e-01, -1.2559e-01,  2.0354e-01,\n",
      "        -4.8287e-03,  2.9083e-01, -9.0240e-02,  2.5741e-01,  2.4190e-01,\n",
      "        -9.4038e-03,  1.7862e-01,  1.1650e-01, -2.1021e-01, -2.3077e-01,\n",
      "         2.5288e-01,  3.1364e-02,  9.1567e-02, -6.0887e-01, -2.5712e-02,\n",
      "         6.6623e-03,  2.1842e-01,  1.1999e-01,  3.5506e-01, -1.1244e-01,\n",
      "         6.4498e-01,  5.0084e-01,  1.1325e-01,  4.2876e-02, -1.3510e-01,\n",
      "         3.5922e-02,  2.0118e-01,  2.9899e-01, -9.8058e-02,  3.5597e-01,\n",
      "         3.3700e-01, -7.7859e-01, -9.8116e-02, -1.9809e-01,  5.6532e-02,\n",
      "        -1.5500e-01, -2.8466e-02,  1.2808e-01, -1.3726e+00,  1.1217e-01,\n",
      "         3.1546e-01, -7.3595e-03, -9.2769e-03, -1.7306e-01, -1.2470e-01,\n",
      "        -1.1269e-01, -1.7430e-01,  2.2193e-02, -1.1070e-01, -1.3966e-02,\n",
      "        -2.5958e-01,  1.1928e-01, -2.4908e-01, -2.8004e-01,  8.2682e-02,\n",
      "         7.5581e-02, -2.5403e-02,  1.6315e-03,  2.1610e-01, -3.9928e-02,\n",
      "         4.0535e-01, -2.3552e-01, -3.8967e-01,  1.6511e-01,  6.6562e-01,\n",
      "        -2.6067e-01, -3.4595e-01, -1.6187e-01, -1.0058e-01, -2.9719e-01,\n",
      "        -2.4153e-01,  5.1243e-02, -8.9320e-02, -7.0011e-02, -6.7813e-01,\n",
      "        -1.1771e-01,  1.7268e-02,  3.9156e-01, -6.2763e-02, -9.7464e-02,\n",
      "        -2.7545e-01, -1.9345e-01, -8.8224e-02,  7.4457e-02,  5.4757e-01,\n",
      "        -3.5556e-01, -3.4301e-01, -7.7844e-02, -4.4770e-01,  2.0383e-01,\n",
      "        -6.0922e-01,  8.2670e-02, -1.7214e-01,  2.4988e-01,  2.4979e-01,\n",
      "         1.1540e-01,  3.2401e-02,  7.0112e-01, -2.1027e-01, -8.7421e-02,\n",
      "         4.8230e-02,  2.1806e-01,  1.1926e-01,  4.1521e-01,  2.5026e-01,\n",
      "         3.1361e-01,  5.3653e-02, -1.7438e-01, -6.6408e-01, -1.2010e-01,\n",
      "         3.0932e-02,  1.0567e-02, -3.3465e-01, -2.1184e-01,  4.8069e-01,\n",
      "        -7.2043e-02, -2.9024e-02, -2.4800e-03, -3.5328e-02, -6.1456e-02,\n",
      "        -8.0110e-02, -1.3097e-01,  1.1884e-01, -6.0206e-02,  1.2978e-02,\n",
      "        -1.8801e-01, -6.8834e-01, -7.0462e-01,  1.5072e-01, -2.4532e-01,\n",
      "         1.9956e-01,  3.5856e-01, -5.3433e-01,  2.8565e-02,  2.5936e-01,\n",
      "        -2.6047e-02,  4.2762e-01,  1.1618e-01, -3.8711e-01,  2.4206e-01,\n",
      "        -3.1411e-01, -6.5187e-02,  7.8882e-02, -2.7209e-02,  4.7044e-02,\n",
      "        -2.7063e-01,  1.4683e-01, -4.3129e-02,  4.5733e-01,  3.0880e-01,\n",
      "         8.7867e-01, -3.4561e-02,  1.5903e-01,  1.0703e-01,  2.1701e-01,\n",
      "        -1.3900e-01, -1.1002e-01, -2.2214e-01,  4.8847e-01,  1.7822e-01,\n",
      "         1.7409e-02, -3.6962e-03,  5.4297e-01,  2.1603e-01,  3.3276e-02,\n",
      "        -9.4673e-02, -2.3482e-01,  6.0436e-02,  1.8537e-02, -1.2108e-01,\n",
      "        -1.5265e-02, -9.3001e-03,  1.6711e-03,  3.0322e-01, -2.4696e-01,\n",
      "        -2.9347e-01, -8.6665e-02,  2.8321e-01, -5.1461e-02, -7.6598e-02,\n",
      "         9.7691e-02, -1.5710e-01, -4.0904e-02,  3.5373e-01, -2.9995e-02,\n",
      "        -8.4518e-02,  3.6678e-01,  3.2701e-01,  6.6750e-02, -5.2386e-02,\n",
      "         2.8497e-01, -1.8313e-01,  2.0933e-01,  2.4550e-01, -1.0669e-01,\n",
      "        -1.4739e-01, -4.0711e-01, -2.6888e-01, -1.8141e-01,  2.6997e-01,\n",
      "        -2.7722e-02, -1.9944e-01,  4.4747e-01, -2.0743e-01, -1.3032e-01,\n",
      "         3.4920e-01, -1.4802e-01, -5.7428e-01, -1.8689e-01,  1.6954e-01,\n",
      "         1.3089e-01,  2.5321e-01, -4.8167e-01,  3.8093e-01,  9.7362e-02,\n",
      "        -3.0499e-02, -2.5573e-01, -4.2723e-01, -2.4171e-01, -9.2343e-01,\n",
      "         8.6327e-03, -3.0807e-02, -3.2552e-01, -7.3647e-01, -3.3687e-01,\n",
      "         1.6695e-01, -3.5714e-01,  1.7452e-01, -1.1619e-01, -7.4833e-02,\n",
      "        -2.8971e-01, -1.4351e-01,  3.7810e-01,  3.6031e-02,  2.1000e-01,\n",
      "         3.7484e-01, -7.3629e-02,  4.3840e-01,  8.3962e-02,  1.6791e-01,\n",
      "         1.5747e-01,  3.5963e-01,  9.2322e-02,  1.0227e-02,  2.4072e-01,\n",
      "         5.4595e-02,  3.9969e-03, -2.8549e-01, -3.2093e-01, -2.2169e-02,\n",
      "         1.5651e-01, -2.5670e-01,  3.4027e-01,  1.1267e-04,  3.2366e-02,\n",
      "         8.1914e-02,  4.0524e-01,  2.7399e-01, -3.2564e-02, -2.2957e-01,\n",
      "        -1.2364e-01, -1.1789e-01,  2.4835e-01, -5.5044e-01,  1.6838e-01,\n",
      "         4.0886e-01,  8.1239e-02,  4.6665e-01, -2.7831e-01,  1.8696e-02,\n",
      "         2.7386e-01,  1.5893e-01,  4.4910e-02, -5.5923e-01,  7.6112e-02,\n",
      "         7.9629e-02,  5.6863e-01,  5.7176e-01, -3.0458e-01, -2.2324e-01,\n",
      "         3.7065e-01, -5.2445e-01, -7.0807e-02,  1.2881e-01,  5.2343e-03,\n",
      "        -5.7318e-02,  5.6462e-01,  1.3415e-01,  8.7897e-02,  5.1912e-01,\n",
      "        -1.1508e-01,  2.6614e-02,  3.6910e-01,  6.3805e-02, -1.6453e-01,\n",
      "         1.5186e-01, -2.2338e-01,  7.4727e-02, -1.2628e-01, -2.5528e-01,\n",
      "         9.9545e-02, -5.0992e-02, -3.5222e-01, -1.6471e-01, -1.5708e-01,\n",
      "        -1.2270e-01,  1.2834e-01, -1.5122e-01,  8.8191e-02, -3.8102e-01,\n",
      "        -3.7497e-03, -2.5662e-01,  1.3708e-01,  4.1296e-01, -2.3790e-01,\n",
      "         1.0342e-01, -7.6833e-01,  2.1985e-01, -5.3382e-01, -3.5804e-01,\n",
      "        -1.4910e-01, -2.7476e-01, -8.0266e-02, -1.3109e-02,  1.0176e-01,\n",
      "         3.6100e-01, -2.3086e-01,  9.2021e-02, -2.2698e-01,  3.6325e-01,\n",
      "         4.8925e-02,  3.0830e-01, -5.0870e-01, -3.7333e-01,  5.3946e-01,\n",
      "         1.5101e-02, -1.4219e-01, -6.8413e-01,  1.2440e-01,  1.7937e-01,\n",
      "         1.1405e-01, -3.1706e-02,  1.7148e-01,  4.7062e-02,  9.4904e-02,\n",
      "        -2.2991e-01, -1.6619e+00,  3.3886e-02, -2.0917e-01,  3.2210e-01,\n",
      "         3.2973e-01, -1.1115e-01,  6.1255e-01, -1.0972e-02,  5.4204e-02,\n",
      "         2.1305e-01, -2.8774e-01, -5.3757e-02,  8.4565e-02, -4.5441e-02,\n",
      "         4.5434e-02,  1.0202e-01, -1.3788e-01,  3.5555e-01, -4.2803e-01,\n",
      "        -3.1673e-01,  2.2733e-01, -3.4027e-01, -1.0035e-01,  7.9386e-02,\n",
      "         6.2322e-02, -3.2607e-01,  5.3585e-01,  2.0708e-01,  1.1234e-01,\n",
      "        -4.3012e-01, -9.6521e-01, -7.2556e-02, -5.9519e-01,  2.7774e-01,\n",
      "        -6.5342e-02,  2.0584e-01,  1.0762e-01])\n"
     ]
    }
   ],
   "source": [
    "print(sent_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.plot_embedding_hist(sent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-212fa85d83b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = 'minimalsample'\n",
    "NAME_TO_VISUALISE_VARIABLE = \"example_embeddings\"\n",
    "path_for_mnist_metadata =  'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings_np = torch.stack(concatenated_lats_4_layers).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_var = tf.Variable(token_embeddings_np, name=NAME_TO_VISUALISE_VARIABLE)\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c8de7462ac7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProjectorConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Specify where you find the metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'projector' is not defined"
     ]
    }
   ],
   "source": [
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Specify where you find the metadata\n",
    "embedding.metadata_path = path_for_mnist_metadata #'metadata.tsv'\n",
    "\n",
    "# Say that you want to visualise the embeddings\n",
    "projector.visualize_embeddings(summary_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_for_mnist_metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b139bf9ff9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_for_mnist_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index\\tLabel\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d\\t %s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_for_mnist_metadata' is not defined"
     ]
    }
   ],
   "source": [
    "with open(path_for_mnist_metadata,'w') as f:\n",
    "    f.write(\"Index\\tLabel\\n\")\n",
    "    for index,label in enumerate(tokenized_text):\n",
    "        f.write(\"%d\\t %s\\n\" % (index,label.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
